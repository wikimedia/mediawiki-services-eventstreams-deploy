{
  "_args": [
    [
      {
        "raw": "kafka-sse@git+https://phabricator.wikimedia.org/diffusion/WKSE/kafkasse.git#v0.0.6",
        "scope": null,
        "escapedName": "kafka-sse",
        "name": "kafka-sse",
        "rawSpec": "git+https://phabricator.wikimedia.org/diffusion/WKSE/kafkasse.git#v0.0.6",
        "spec": "https://phabricator.wikimedia.org/diffusion/WKSE/kafkasse.git#v0.0.6",
        "type": "git"
      },
      "/opt/service"
    ]
  ],
  "_from": "git+https://phabricator.wikimedia.org/diffusion/WKSE/kafkasse.git#v0.0.6",
  "_id": "kafka-sse@0.0.6",
  "_inCache": true,
  "_location": "/kafka-sse",
  "_phantomChildren": {},
  "_requested": {
    "raw": "kafka-sse@git+https://phabricator.wikimedia.org/diffusion/WKSE/kafkasse.git#v0.0.6",
    "scope": null,
    "escapedName": "kafka-sse",
    "name": "kafka-sse",
    "rawSpec": "git+https://phabricator.wikimedia.org/diffusion/WKSE/kafkasse.git#v0.0.6",
    "spec": "https://phabricator.wikimedia.org/diffusion/WKSE/kafkasse.git#v0.0.6",
    "type": "git"
  },
  "_requiredBy": [
    "/"
  ],
  "_resolved": "git+https://phabricator.wikimedia.org/diffusion/WKSE/kafkasse.git#92eaffafcf7193628281e158927479b20c84011a",
  "_shasum": "d75bf0734630ff99d1a0c8c32edb50e7b0e1ec7f",
  "_shrinkwrap": null,
  "_spec": "kafka-sse@git+https://phabricator.wikimedia.org/diffusion/WKSE/kafkasse.git#v0.0.6",
  "_where": "/opt/service",
  "author": {
    "name": "Andrew Otto",
    "email": "otto@wikimedia.org"
  },
  "bugs": {
    "url": "https://phabricator.wikimedia.org/search/query/fpxAPkMeWqjh/"
  },
  "dependencies": {
    "bluebird": "^3.4.3",
    "bunyan": "^1.8.1",
    "lodash": "^4.15.0",
    "node-rdkafka": "^0.6.0",
    "node-uuid": "^1.4.7",
    "safe-regex": "^1.1.0"
  },
  "description": "KafkaSSE - Kafka Consumer to HTTP SSE/EventSource",
  "devDependencies": {
    "coveralls": "^2.11.11",
    "eventsource": "^0.2.1",
    "istanbul": "^0.4.4",
    "jscs": "^3.0.7",
    "mocha": "^2.5.3",
    "mocha-jscs": "^5.0.1",
    "mocha-jshint": "^2.3.1",
    "mocha-lcov-reporter": "^1.2.0",
    "node-fetch": "^1.6.3",
    "nsp": "^2.6.1",
    "sinon": "^1.17.6"
  },
  "gitHead": "92eaffafcf7193628281e158927479b20c84011a",
  "homepage": "https://github.com/wikimedia/kafkasse#readme",
  "keywords": [
    "kafka",
    "http",
    "sse",
    "eventsource",
    "stream"
  ],
  "license": "Apache-2.0",
  "main": "index.js",
  "name": "kafka-sse",
  "optionalDependencies": {},
  "readme": "# KafkaSSE\n\n[![Travis Build Status](https://travis-ci.org/wikimedia/KafkaSSE.svg?branch=master)](https://travis-ci.org/wikimedia/KafkaSSE)\n[![Coveralls](https://coveralls.io/repos/github/wikimedia/KafkaSSE/badge.svg?branch=master)](https://coveralls.io/github/wikimedia/KafkaSSE?branch=master)\n\nKafka Consumer to HTTP SSE/EventSource\n\nUses [node-rdkafka](https://github.com/Blizzard/node-rdkafka) KafkaConsumer to\nstream JSON messages to clients over HTTP in [SSE/EventSource format](https://www.w3.org/TR/eventsource/)\n\nThe `Last-Event-ID` and EventSource `id` field will be used to handle auto-resume\nduring client disconnects.  By using EventSource to connect to a KafkaSSE endpoint,\nyour client will automatically resume from where it left off if it gets disconnected\nfrom the server.  Every message sent to the client will have an `id` field that will be\na JSON array of objects describing each latest topic, partition and offset seen by this client.\nOn a reconnect, this object will be sent back as the `Last-Event-ID` header, and will be used\nby KafkaSSE to assign a KafkaConsumer to start from those offsets.\n\nSee also [Kasocki](https://github.com/wikimedia/kasocki).\n\n## Usage\n\n### HTTP server KafkaSSE set up\nThe kafka-sse module exports a function that wraps up handling an HTTP SSE request for Kafka topics.\n\n```javascript\n'use strict';\nconst kafkaSse = require('kafka-sse');\nconst server = require('http').createServer();\n\nconst options = {\n    kafkaConfig: {'metadata.broker.list': 'mybroker:9092'}\n}\n\nserver.on('request', (req, res) => {\n    const topics = req.url.replace('/', '').split(',');\n    console.log(`Handling SSE request for topics ${topics}`);\n    kafkaSse(req, res, topics, options)\n    // This won't happen unless client disconnects or kafkaSse encounters an error.\n    .then(() => {\n        console.log('Finished handling SSE request.');\n    });\n});\n\nserver.listen(6917);\nconsole.log('Listening for SSE connections at http:/localhost:6917/:topics');\n```\n\n\n### Custom deserializer\n\nThe default deserializer used for messages returned from node-rdkafka assumes\nthat `kafkaMessage.value` is a utf-8 byte buffer containing a JSON string.  It parses\n`kafkaMessage.value` into an object, and then sets it as `kafkaMessage.message`.\n`kafkaMessage.message` is what will be sent to the connected SSE client as an\nevent.\n\nYou may override this default deserializer.  The deserializer is given the `kafkaMessage` as\nreturned by node-rdkafka `consume`.  You must make sure to set the `message` field on this\nobject, and not modify the other top level fields such as `topic`, `offset` and `partition`.\nThese are used to set the `Last-Event-ID` header.\n\n```javascript\nfunction customDeserializer(kafkaMessage) {\n    kafkaMessage.message = JSON.parse(kafkaMessage.value.toString());\n    kafkaMessage.message.extraInfo = 'I was deserialized by a custom deserializer';\n    return kafkaMesssage;\n}\n\n//...\n\nkafkaSse(req, res, topics, {\n    deserializer: customDeserializer,\n});\n\n// ...\n```\n\n### Server Side filtering\n\nBy default, all consumed messages are sent to the client.  However, you may provide\na custom filter function as the `filterer` option.\nThis function will be given the `kafkaMessage` as returned\nby the `deserializer` function.  The message will be kept and sent to the client if your\nfilter function returns true, otherwise it will be skipped.\n\n```javascript\n/**\n * Only send events to SSE clients that have `price` field greater than `10.0`;\n */\nfunction filterFunction(kafkaMessage) {\n    return kafkaMessage.message.price >= 10.0;\n}\n\n//...\n\nkafkaSse(req, res, topics, {\n    filterer: filterFunction,\n});\n\n// ...\n```\n\n### NodeJS EventSource usage\n```javascript\nconst EventSource = require('eventsource');\n'use strict';\nconst topics = process.argv[2];\nconst port   = 6917\n\nconst url = `http://localhost:${port}/${topics}`;\nconsole.log(`Connecting to Kafka SSE server at ${url}`);\nlet eventSource = new EventSource(url);\n\neventSource.onopen = function(event) {\n    console.log('--- Opened SSE connection.');\n};\n\neventSource.onerror = function(event) {\n    console.log('--- Got SSE error', event);\n};\n\neventSource.onmessage = function(event) {\n    // event.data will be a JSON string containing the message event.\n    console.log(JSON.parse(event.data));\n};\n```\n\n\n## Errors\n\nIf an error is encountered during SSE client connection, a normal HTTP error response\nwill be returnred, along with JSON information about the error in the response body.\nHowever, once the SSE connection has started, the HTTP response header will have already\nbeen set to 200.  From that point on, errors are given to the client as `onerror` EventSource\nevents.  You must register an `onerror` function for your EventSource object to receive these.\n\n\n## Notes on Kafka consumer state\n\nIn normal use cases, Kafka (and previously Zookeeper) handles consumer state.\nKafka keeps track of multiple consumer processes in named consumer groups, and\nhandles rebalancing of those processes as they come and go.  Kafka also\nhandles offset commits, keeping track of the high water mark each consumer\nhas reached in each topic and partition.\n\nKafkaSSE is intended to be exposed to the public internet by enabling\nweb based consumers to use HTTP to consume from Kafka.  Since\nthe internet at large cannot be trusted, we would prefer to avoid allowing\nthe internet to make any state changes to our Kafka clusters.  KakfaSSE\npushes as much consumer state management to the connected clients as it can.\n\nOffset commits are not supported.  Instead, latest subscription state is sent\nas the EventSource `id` field with each event.  This information can be\nused during connection initializion in the `Last-Event-ID` header\nto specify the positions at which KafkaSSE should start consuming from Kafka.\n`Last-Event-ID` should be an assignments array, of the form:\n\n```javascript\n[\n    { topic: 'topicA', partition: 0, offset 12345 },\n    { topic: 'topicB', partition: 0, offset 46666 },\n    { topic: 'topicB', partition: 1, offset 45555 },\n]\n```\n\nConsumer group management is also not supported.  Each new SSE client\ncorresponds to a new consumer group.  There is no way to parallelize\nconsumption from Kafka for a single connected client.  Ideally, we would not\nregister a consumer group at all with Kafka, but as of this writing\n[librdkafka](https://github.com/Blizzard/node-rdkafka/issues/18) and\n[blizzard/node-rdkafka](https://github.com/Blizzard/node-rdkafka/issues/18)\ndon't support this yet.  Consumer groups that are registered with Kafka\nare named after the `x-request-id` header, or a uuid if this is not set, e.g.\n`KafkaSSE-2a360ded-1da0-4258-bad5-90ce954b7c52`.\n\n## node-rdkafka consume modes\nThe node-rdkafka client that KafkaSSE uses has\n[several consume APIs](https://github.com/Blizzard/node-rdkafka#kafkakafkaconsumer).\nKafkaSSE uses the [Standard Non flowing API](https://github.com/Blizzard/node-rdkafka#standard-api-1).\n\n\n## Testing\nMocha tests require a running 0.9+ Kafka broker at `localhost:9092` with\n`delete.topic.enable=true`.  `test/utils/kafka_fixture.sh` will prepare\ntopics in Kafka for tests.  `npm test` will download, install, and run\na Kafka broker.  If you already have one running locally, then\n`npm run test-local` will be easier to run.  You may set the `KAFKA_TOPICS_CMD` and\nthe `KAFKA_CONSOLE_PRODUCER_CMD` environment variables if you would like to override\nthe commands used in`kafka_fixtue.sh`.\n\n## To Do\n\n- tests for kafkaEventHandlers\n",
  "readmeFilename": "README.md",
  "repository": {
    "type": "git",
    "url": "https://phabricator.wikimedia.org/diffusion/WKSE/kafkasse.git"
  },
  "scripts": {
    "coverage": "export UV_THREADPOOL_SIZE=128 && ./node_modules/istanbul/lib/cli.js cover _mocha -- -R spec",
    "coveralls": "cat ./coverage/lcov.info | ./node_modules/coveralls/bin/coveralls.js",
    "kafka-fixture": "./test/utils/kafka_fixture.sh",
    "kafka-install": "./test/utils/kafka_install.sh",
    "kafka-start": "./test/utils/kafka.sh start",
    "kafka-stop": "./test/utils/kafka.sh stop",
    "mocha": "export UV_THREADPOOL_SIZE=128; mocha",
    "start": "./server.js | ./node_modules/bunyan/bin/bunyan",
    "test": "npm run test-jenkins",
    "test-jenkins": "npm run kafka-install && npm run kafka-stop && npm run kafka-start && npm run kafka-fixture && npm run coverage && npm run kafka-stop",
    "test-local": "npm run kafka-fixture && npm run coverage",
    "test-travis": "npm run kafka-install && npm run kafka-start && npm run kafka-fixture && npm run coverage && npm run coveralls"
  },
  "version": "0.0.6"
}
